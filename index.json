[{"authors":["admin"],"categories":null,"content":" I am a third-year PhD student at CREST - ENSAE, advised by Victor-Emmanuel Brunel.\nNews: I defended my PhD thesis on December 15, 2023. My manuscript is available here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1587077318,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a third-year PhD student at CREST - ENSAE, advised by Victor-Emmanuel Brunel.\nNews: I defended my PhD thesis on December 15, 2023. My manuscript is available here.","tags":null,"title":"","type":"authors"},{"authors":["Gabriel Romon","Victor-Emmanuel Brunel"],"categories":[],"content":"","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697988873,"objectID":"91b10be371fff8970642bdaa8233285d","permalink":"/publication/romon-2023-tree/","publishdate":"2023-10-22T15:34:33.845935Z","relpermalink":"/publication/romon-2023-tree/","section":"publication","summary":"We are interested in measures of central tendency for a population on a network, which is modeled by a metric tree. The location parameters that we study are generalized Fréchet means obtained by minimizing the objective function $\\alpha \\mapsto \\mathbb E[\\ell(d(\\alpha,X))]$ where $\\ell$ is a generic convex nondecreasing loss.\n\nWe leverage the geometry of the tree and the geodesic convexity of the objective to develop a notion of directional derivative in the tree, which helps up locate and characterize the minimizers.\n\nEstimation is performed using a sample analog. We extend to a metric tree the notion of stickiness defined by Hotz et al. (2013), we show that this phenomenon has a non-asymptotic component and we obtain a sticky law of large numbers. For the particular case of the Fréchet median we develop non-asymptotic concentration bounds and sticky central limit theorems.","tags":[],"title":"Convex generalized Fréchet means in a metric tree","type":"publication"},{"authors":["Gabriel Romon"],"categories":[],"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667322926,"objectID":"7404089ee2ead4e21c6424e751bdf4c6","permalink":"/publication/romon-2022-quantiles/","publishdate":"2022-11-01T17:15:25.961364Z","relpermalink":"/publication/romon-2022-quantiles/","section":"publication","summary":"Geometric quantiles are location parameters which extend classical univariate quantiles to normed spaces (possibly infinite-dimensional) and which include the geometric median as a special case. The infinite-dimensional setting is highly relevant in the modeling and analysis of functional data, as well as for kernel methods.\n\nWe begin by providing new results on the existence and uniqueness of geometric quantiles. Estimation is then performed with an approximate M-estimator and we investigate its large-sample properties in infinite dimension.\n\nWhen the population quantile is not uniquely defined, we leverage the theory of variational convergence to obtain asymptotic statements on subsequences in the weak topology. When there is a unique population quantile, we show, under minimal assumptions, that the estimator is consistent in the norm topology for a wide range of Banach spaces including every separable uniformly convex space.\n\nIn separable Hilbert spaces, we establish weak Bahadur--Kiefer representations of the estimator. As a consequence, we obtain the first central limit theorem valid in a generic Hilbert space and under minimal assumptions that exactly match those of the finite-dimensional case.\n\nOur consistency and asymptotic normality results significantly improve the state of the art, even for exact geometric medians in Hilbert spaces.","tags":[],"title":"Statistical properties of approximate geometric quantiles in infinite-dimensional Banach spaces","type":"publication"},{"authors":["Kai Tan","Gabriel Romon","Pierre C Bellec"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658423726,"objectID":"3b8e288f85dc40c3cc826402f08dcf0b","permalink":"/publication/tan-2022-noise/","publishdate":"2022-06-15T17:15:25.961364Z","relpermalink":"/publication/tan-2022-noise/","section":"publication","summary":"This paper studies the multi-task high-dimensional linear regression models where the noise among different tasks is correlated, in the moderately high dimensional regime where sample size n and dimension p are of the same order. Our goal is to estimate the covariance matrix of the noise random vectors, or equivalently the correlation of the noise variables on any pair of two tasks. Treating the regression coefficients as a nuisance parameter, we leverage the multi-task elastic-net and multi-task lasso estimators to estimate the nuisance. By precisely understanding the bias of the squared residual matrix and by correcting this bias, we develop a novel estimator of the noise covariance that converges in Frobenius norm at the rate $n^{−1/2}$ when the covariates are Gaussian. This novel estimator is efficiently computable.\n\nUnder suitable conditions, the proposed estimator of the noise covariance attains the same rate of convergence as the \"oracle\" estimator that knows in advance the regression coefficients of the multi-task model. The Frobenius error bounds obtained in this paper also illustrate the advantage of this new estimator compared to a method-of-moments estimator that does not attempt to estimate the nuisance. As a byproduct of our techniques, we obtain an estimate of the generalization error of the multi-task elastic-net and multi-task lasso estimators. Extensive simulation studies are carried out to illustrate the numerical performance of the proposed method.","tags":[],"title":"Noise Covariance Estimation in Multi-Task High-dimensional Linear Models","type":"publication"},{"authors":["Pierre C Bellec","Gabriel Romon"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626887726,"objectID":"04b4fc277ff81a90fd0f5fa0729692ad","permalink":"/publication/bellec-2021-chisquare/","publishdate":"2021-07-21T17:15:25.961364Z","relpermalink":"/publication/bellec-2021-chisquare/","section":"publication","summary":"The paper proposes chi-square and normal inference methodologies for the unknown coefficient matrix $B^\\*$ of size $p\\\\times T$ in a Multi-Task (MT) linear model with $p$ covariates, $T$ tasks and $n$ observations under a row-sparse assumption on $B^\\*$. The row-sparsity $s$, dimension $p$ and number of tasks $T$ are allowed to grow with $n$. In the high-dimensional regime $p\\\\ggg n$, in order to leverage row-sparsity, the MT Lasso is considered.\n\nWe build upon the MT Lasso with a de-biasing scheme to correct for the bias induced by the penalty. This scheme requires the introduction of a new data-driven object, coined the interaction matrix, that captures effective correlations between noise vector and residuals on different tasks. This matrix is psd, of size $T\\\\times T$ and can be computed efficiently.\n\nThe interaction matrix lets us derive asymptotic normal and $\\\\chi^2\\_T$ results under Gaussian design and $\\\\frac{sT+s\\\\log(p/s)}{n}\\\\to0$ which corresponds to consistency in Frobenius norm. These asymptotic distribution results yield valid confidence intervals for single entries of $B^\\*$ and valid confidence ellipsoids for single rows of $B^\\*$, for both known and unknown design covariance $\\\\Sigma$. While previous proposals in grouped-variables regression require row-sparsity $s\\\\lesssim\\\\sqrt n$ up to constants depending on $T$ and logarithmic factors in $n,p$, the de-biasing scheme using the interaction matrix provides confidence intervals and $\\\\chi^2\\_T$ confidence ellipsoids under the conditions ${\\\\min(T^2,\\\\log^8p)}/{n}\\\\to 0$ and $$ \\\\frac{sT+s\\\\log(p/s)+\\\\|\\\\Sigma^{-1}e\\\\_j\\\\|\\\\_0\\\\log p}{n}\\\\to0, \\\\quad \\\\frac{\\\\min(s,\\\\|\\\\Sigma^{-1}e\\\\_j\\\\|\\\\_0)}{\\\\sqrt n} \\\\sqrt{[T+\\\\log(p/s)]\\\\log p}\\\\to 0, $$ allowing row-sparsity $s\\\\ggg\\\\sqrt n$ when $\\\\|\\\\Sigma^{-1}e\\_j\\\\|\\_0 \\\\sqrt T\\\\lll \\\\sqrt{n}$ up to logarithmic factors.","tags":[],"title":"Chi-square and normal inference in high-dimensional multi-task regression","type":"publication"},{"authors":null,"categories":null,"content":"I was a Teaching Assistant for the following courses at ENSAE Paris.\n2022-2023\nProbability Theory (30 hours)\nAdvanced Machine Learning (9 hours)\nFunctional and Convex Analysis (27 hours)\n2021-2022\nProbability Theory (30 hours)\nAdvanced Machine Learning (9 hours)\nFunctional and Convex Analysis (27 hours)\n2020-2021\nProbability Theory (30 hours)\nApplied Statistical Learning (6 hours)\nFunctional and Convex Analysis, taught by Vahagn Nersesyan (27 hours)\n2019-2020\nPython for Data Scientists (12 hours)\nmaterial\nIntroduction to Python (6 hours)\nmaterial\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"322dbaccf72a6d71f827fdb2866be935","permalink":"/teaching/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"My TA duties","tags":null,"title":"Teaching","type":"page"},{"authors":null,"categories":null,"content":"Valeur absolue\nSuites\nTrigonométrie\nExponentielle\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f37ef44930b44696d6870a9e84d84830","permalink":"/tutoring/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutoring/","section":"","summary":"Valeur absolue\nSuites\nTrigonométrie\nExponentielle","tags":null,"title":"Tutoring","type":"page"}]